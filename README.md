# Продвинутые задачи и модели NLP. Морфология
Данный курс посвящен основным задачам морфологии и способам их решения.
Старые [презентации](https://drive.google.com/drive/u/0/folders/0B07mCd0ZUKkjb1lCMmxrQXlYNGc) Ильи Гусева.
Предполагается, что студенты уже знакомы с **pytorch** и умеют писать нейроные сети.

## Структура курса
<!-- TOC -->
- [Основные понятия](#Основные-понятия)
- [Морфологический анализ](#Морфологический-анализ)
- [Исправление ошибок](#Исправление-ошибок)
- Морфологический синтез
<!-- /TOC -->

Будут рассмотрены state-of-the-art решения для следующих задач:
- [Part-of-speech tagging](http://nlpprogress.com/english/part-of-speech_tagging.html)
- [Grammatical error correction](http://nlpprogress.com/english/grammatical_error_correction.html)
- [Lexical normalization](http://nlpprogress.com/english/lexical_normalization.html)

<div style="page-break-after: always;"></div> 

## Основные понятия
**Морфология** - раздел грамматики, основными объектами которого являются  **слова** естественных языков, их значимые части и морфологические признаки.

<img src="pics/pyramid.PNG" height="300" style="float:right">

#### Морфология изучает:
- Словоизменение
- Словообразование 
- Части речи
- Грамматические значения

<div style="page-break-after: always;"></div> 

## Грамматическое значение

Значение, выражаемое словоизменительной **морфемой** 

**Морфема** - наименьшая единица языка, имеющая некоторый смысл. 

Морфемы подразделяются на два основных типа - **корневые** и **аффиксальные**

**Корень** - основная значимая часть слова.

**Аффикс** - вспомогательная часть слова, присоединяемая к корню и служащая для словообразования и выражения грамматических значений.

#### Типы аффиксов
- **Префиксы** (приставки в русском), 
**постфиксы** (суффиксы и окончания в русском), 
**интерфиксы** (для связи корней в сложны словах), 
**инфиксы** (вставляются в середину корня в индонезийских языках) 
и **трансфиксы** (разрывают корень из согласных гласными, например в арабском языке)
- **Словообразующие** (деривационные), которые образуют новые слова с новыми значениями (суффиксы и приставки в русском),
 и **формообразующие** (реляционные), которые образуют формы слов, выражая одну или несколько грамматических категорий 
 и указывая на связь с другими членами предложения (в флектиыных языках, в т.ч. русском такие морфемы называют флексиями).   

<div style="page-break-after: always;"></div> 

## Граммемы и грамматические категории
**Грамматическая категория** -  замкнутая система взаимоисключающих и противопоставленных друг другу грамматических значений (граммем),
 задающая разбиение обширной совокупности словоформ (или небольшого набора высокочастотных словоформ с абстрактным типом значения) на непересекающиеся классы,
 различие между которыми существенно сказывается на степени грамматической правильности текста.

#### Примеры грамматических категорий
- **Именные**: падеж, род, определенность, одушевленность, личность 
- **Глагольные**: время, вид, лицо, наклонение, залог
- **"Оба"** - число

**Граммема** -  грамматическое значение, понимаемое как один из элементов грамматической категории;
 различные граммемы одной категории исключают друг друга и не могут быть выражены вместе.

#### Примеры граммем:
- Единственное и множественное число
- Мужской, женский и средний род 
- Именительный, родительный и т.д. падежи

Следовательно **грамматическое значение** - набор граммем всех категорий, характерных для данной части речи. 
Например, для “ежа”: существительное, одушевлённое, мужской род, единственное число, родительный падеж.

<div style="page-break-after: always;"></div>

## Грамматическое и лексическое значение
- Грамматические значения образуют замкнутый более структурируемый класс.
- Грамматические значения выражаются в принудительном порядке. (У существительно всегда есть категории числа, рода, падежа)
- Лексические и грамматические значения отличаются с точки зрения способов и средств их формального выражения. 
(Грамматические значения выражаются с помощью реляционных морфем)
- Грамматические значения могут не иметь полного соответствия во внеязыковой сфере. (род существительных *табуретка* и  *стул* мотивированы их окончаниями)

### Характерные размеры грамматических систем
#### Для одного языка:
- ~30 грамматических категорий
- До 90 граммем на категорию
- В среднем 4-5 граммем на категорию

#### Для одной части речи:
- В среднем до 4 грамматических категорий
- До 20 грамматических категорий

<div style="page-break-after: always;"></div> 

## Глоссарий
- **Вокабула**:
  - Заголовок словарной статьи; 
  - Слово иностранного языка с переводом на родной язык; 
  - Графическое слово
- **Лемма** - нормальная (словарная) форма слова. Для русского языка характерно:
  - для существительных - именительный падеж, единственное число
  - для прилагательных - именительные падеж, единственное число, мужской род
  - для глаголов, причастий и деепричастий - глагол в инфинитиве несовершенного вида
- **Словоформа** - слово с определенной леммой и грамматическим значением. *"Ворон"* и *"(к) ворону"* - разные словофромы одной лексемы.
- **Лексема** - набор всех словоформ слова. Т.е. *"Ворон"*, *"(к) ворону"*, *"вороном"*, *"(о) вороне"*, *"вороны"* и т.д.

<div style="page-break-after: always;"></div> 

## Словоизменительная парадигма
В лингвистике список словоформ, принадлежащих одной лексеме и имеющих разные грамматические значения.  Обычно представлена в виде таблицы. 
Словоизменительная парадигма выступает образцом того, как строятся словоизменительные формы для целых классов лексем (склонений существительных, спряжений глаголов и т. п.)

<table align="center">
<tr><td>

||Ед. ч.|Мн. ч|
|:---|:---:|:---:|
|**И**|рук<span style="color:red">а</span>|рук<span style="color:red">и</span>|
|**Р**|рук<span style="color:red">и</span>|рук|
|**Д**|рук<span style="color:red">е</span>|рук<span style="color:red">ам</span>|
|**В**|рук<span style="color:red">у</span>|рук<span style="color:red">и</span>|
|**Т**|рук<span style="color:red">ой</span>|рук<span style="color:red">ами</span>|
|**П**|рук<span style="color:red">е</span>|рук<span style="color:red">ах</span>|

</td><td>

||Ед. ч.|Мн. ч|
|:---|:---:|:---:|
|**И**|тубус|тубус<span style="color:red">ы</span>|
|**Р**|тубус<span style="color:red">а</span>|тубус<span style="color:red">ов</span>|
|**Д**|тубус<span style="color:red">у</span>|тубус<span style="color:red">ам</span>|
|**В**|тубус|тубус<span style="color:red">ы</span>|
|**Т**|тубус<span style="color:red">ом</span>|тубус<span style="color:red">ами</span>|
|**П**|тубус<span style="color:red">е</span>|тубус<span style="color:red">ах</span>|

</td><td>

|||
|:---:|:---:|
|читать|тонуть|
|читала|тонула|
|читая|?|
|читаемый|?|
|читанный|?|

</td></tr>
</table>

#### Парадигма определяет:
- неизменяемое, классифицирующее грамматическое значение лексемы;
- какие есть формы у лексемы;
- как записываются формы лексемы.

<div style="page-break-after: always;"></div> 

## Словообразовательная парадигма
Совокупность производных слов (дериватов) от одной основы.
- у образованных слов своё словоизменение
- лексемы с одинаковым словоизменением могут по-разному образовывать дериваты

<table align="center">
<tr><td>

||||
|:---:|:---:|:---:|
|слон|слоник|слонёнок|
|краб|крабик|<span style="color:red">крабёнок</span>|

</td><td>

||||
|:---:|:---:|:---:|
|Алёна|<span style="color:red">Алёночка</span>|Алёнушка|
|Лена|Леночка|<span style="color:red">Ленушка</span>|

</td></tr>
</table>

<div style="page-break-after: always;"></div> 

## Классификация словоизменения

#### 1. Изолирующие (аморфные, односложные, корневые) языки 
Языки с низким соотношением морфем к слову. Слова в максимально изолирующем языке будут состоять только из одной морфемы — корня,
 не образуя ни составных слов, ни сочетаний с суффиксами, префиксами и т. д. (Например слово "хаки" в русском). Также изолирующие языки имеют тенденцию избегать служебные слова,
 поэтому порядок в слов в предложении очень важен

 Изолирующие языки распространены в Юго-Восточной Азии:
- Вьетнамский язык
- Классический китайский язык
- Бирманский язык
- Тайский язык
- Кхмерский язык в Камбодже
- Лаосский язык 

#### 2. Аналитические языки
Являются изолирующими, однако синтаксическая информация предаётся при помощи отдельных грамматикализированных слов вместо морфологии. Типичные представители: 
английский, голландский, болгарские языки.

<div style="page-break-after: always;"></div> 

#### 3. Синтетические языки
Типологический класс языков, в которых преобладают синтетические формы выражения грамматических значений.

В синтетических языках грамматические значения выражаются в пределах самого слова (аффиксация, внутренняя флексия, ударение, супплетивизм),
 то есть формами самих слов. Для выражения отношений между словами в предложении могут быть использованы также элементы аналитического строя
 (служебные слова, порядок знаменательных слов, интонация).

#### 4. Флективные языки
Язык синтетического типа, при котором доминирует словоизменение при помощи флексий — формантов, сочетающих сразу несколько значений. 
Типичным примером является русский: 
- стел-ю (НВ+ЕЧ)
- стел-ила (ПВ+ЕЧ+ЖР)
- стел-ись (ЕЧ+Повелит)
- стел-ющийся (Прич+НВ+ЕЧ+МР)
- стел-ясь (Дееприч+НВ)

#### 5. Аглютитативные языки
Язык синтетического типа, в котором доминирующим типом словоизменения является агглютинация
(«приклеивание» различных формантов (суффиксов или префиксов)), причём каждый из них несёт только одно значение.

Пример (турецкий):  **ev**(дом) + **ler**(МнЧ) + **im**(выражаает принадлежность *1Л, ЕЧ*) + **de**(местный падеж) = **evlerimde** (В моих домах); 
**evimde**(В моём доме); **evlerde**(В домах) 
 

<div style="page-break-after: always;"></div> 

## Дополнительно

#### Аблаут (чередование гласных)
"Внутрення флексия": соб<span style="color:red">-</span>рать - соб<span style="color:red">е</span>ру - собир<span style="color:red">а</span>ю

#### Морфема внутри корня, разрывная мофрология
Арабский язык: 
- **k t b** "писать" (корень) 
  - +**-a-a-** Прош.Вр., Акт.залог (огласовки) 
  - +**-a** 3л, Ед.ч, МР (суффикс (окончание)) 
  - =**kataba** "написал"
- **k tt b** Интенсив ~«много писать» (корень) 
  - +**-u-i-**	Прош.Вр., Пассив.залог (огласовки) 
  - +**tu-____-u** 3л, Ед.ч, ЖР., Имперф, пассив (префикс, суффикс) 
  - =**tukuttibu**	 "(её) много писали"

<div style="page-break-after: always;"></div> 

## Задачи компьютерной морфологии
- **Спеллинг** - проверка слова по словарю
- **Морфологический анализ** - определение грамматического значения и леммы у словоформы
- **Морфологический синтез** - построение словоформы по лемме и грамматическому значению
- **Исправление опечаток** - поиск наиболее близкого слова в слоаваре

Далее подробно о каждой задаче и способах её решения.

<div style="page-break-after: always;"></div>

## Морфологический анализ
Морфологический анализ в лингвистике — определение морфологических характеристик слова.

#### Задача морфологического анализа
- Получение леммы
- Получение грамматического значения

Существуют уже готовые морфологические анализаторы, которые справляются с этой задачей. 
Например, **MyStem** и **pymorphy**. Чтобы поближе с ними познакомиться, предлагается изучить [morph_analyzers.ipynb](./morph_analyzers.ipynb)

Как было показано в ноутбуке - данные анализаторы имеют свои недостатки, т.к. сильно опираются на словарь.
Ралее рассмотрим нейросетевые подходы к решению подобной задачи, ограничившись только определением частей речи у слов. 
Данную задачу еще называют **Part-of-speech tagging**. 

<div style="page-break-after: always;"></div>

## POS-tagging
Данная задача относится к типу **Sequence labeling**, т.е. каждому элементу последовательности (В нашем случае слова в тексте)
 нужно сопоставить некоторый тэг, класс (часть речи).

Обычно подобные задачи решаются с помощью рукуррентых сетей, использующих на входе предобученные эмбеддинги слов 
*(предполагается, что Вы с ними уже знакомы)*. Рассмотрим же самую ленивую реализацию подобной модели на **keras** в ноутбуке [pos_keras.ipynb](./pos_keras.ipynb)

Далее мы рассмотрим решение этой же задачи, но уже на чистом **pytorch**.

<div style="page-break-after: always;"></div>

## PyTorch pipeline
Для решения большинства NLP задач на **чистом** торче обычно нужны следующие компоненты:
- **Словарь** - хранит отображение строковых значений, например слов, в индексы.
- **Датасет** - представляет собой контейнер для данных, возвращающий тензор данных по индексу или по батчу.
В торче уже есть базовый класс, от которого можно отнаследоваться и пользоваться торчевыми итераторами.
- **Ридер** - читает данные из файлов и возвращает список объектов (инстансов). Иногда эту роль может выполнять датасет, 
но тогда могут возникнуть проблемы с индексацией.
- **Индексер** - строит словарь (или словари) по датасету. Опять же, если например, нам нужны только токены, то словарь можно инициализировать  эмбеддингами.
- **Модель** - собственно сама модель нейросети. Принимает тензоры, возвращает тензоры. Не стоит в модель пихать еще функцию ошибки.
- **Тренер** - учит модель, проходя по датасету. Тут все стандартно
- **Система** - обертка над моделью, которая считает лосс и метрики, декодирует и прочее.  
- **Утилиты** - различные функции для упрощения жизни, например, загрузка эмбеддингов, выравнивание тензоров и метрики.

Все выше перечисленное - скорее разумное наблюдение, нежели абсолютный гайдлайн. В некоторых случаях можно объеденить эти сущности или же, наоборот, добавить новые.

В качестве примера приглашаю в скрипт [pos_torch.py](./pos_torch.py)

<div style="page-break-after: always;"></div>

## AllenNLP 
К счастью, писать каждый раз столько кода не нужно. От задачи к задаче многие сущности повторяются: словарь, индексер, тренер, утилиты и обертки. 
Даже больше: все это уже реализовано за нас, например в [AllenNLP](https://allennlp.org/).

В этом фреймворке есть следующие сущности:
- `DatasetReader` - читает датасет и возвращает список инстансов `Instance`- контейнеров для входных и выходных данных. 
По сути это отображение строк в поля `Field`, которые представляют собой данные и умеют возвращать тензор.
- `Vocabulary` - словарь, хранящий отображения полей в индексы. Удобен различными способами инициализации: из инстансов или из файлов.
- `Model` - в предудущих понятиях это система - считает лосси метрики, прогоняет сеть и декодирует. В методе `forward` возвращает `Dict[str, torch.Tensot]`, 
в который обычно кладут лосс, логиты или пути, если мы используем CRF. Метод `decode` вызывается после `forward` и модифицирует `Dict`, например возвращая лэйблы.
- `Trainer` - хорошо настареваемый тренер, которому можно указаьб кол-во эпох, валидационную метрику, по которой выбирать лучшую модель, и прочее.

Ну а теперь давайте посмотрим, насколько AllenNLP прекрасен: [pos_allennlp.ipynb](pos_allennlp.ipynb)

<div style="page-break-after: always;"></div>

## AllenNLP JSON

Вы думали это все? Нет, на allennlp можно писать в JSON. Запуск примера:

```
allennlp train pos_config.json -s <путь до папки с результатом> --include-package src_allennlp
 ``` 

<details><summary><b>Бонус</b></summary>
<img src="pics/allennlp.jpg" align="middle">
</details>

<div style="page-break-after: always;"></div>

## Flair embeddings

Теперь разберем один из SOTA примеров: **Flair embeddings**. Данный подход позволяет генерировать контекстуальные "строковые" эмбеддинги. 

Суть довольно проста: мы представляем текст как последовательность символов и эмбеддер (BiRNN) возвращает векторные представления для них.
Эмбеддинги для каждого слов берутся как конкатенация эмбеддингов последнего символа слова при прямом проходе и первого - при обратном.

Энкодер здесь представляет собой BiLSTM-CRF.

<img src="pics/flair.PNG" width="350">

<img src="pics/charlm.PNG" width="450">

*Akbik A., Blythe D., Vollgraf R. Contextual string embeddings for sequence labeling //Proceedings of the 27th International Conference on Computational Linguistics. – 2018. – С. 1638-1649.*

<div style="page-break-after: always;"></div>


## Исправление ошибок
Перейдем к следующей задаче, а именно исправлению ошибок. Одну из главных задач здесь играет словарь, но о нём позже. Сама же задача обычно решается в два этапа:
- Поиск ошибок (**Spelling**) - нахождение слов, в которых возможны опечатки. Самое простое решение - проверка наличия такого слова в словаре.
- Собственно исправление слов или предложение вариантов исправления, например, с помощью поиском ближайших слов (по edit distance) в словаре.  

Зачем всё это нужно:
- Проверка орфографии в текстовых редакторах
- Подсказки при наборе текста
- Поиск. Коррекция запросов
- Распознавание текста и речи

<div style="page-break-after: always;"></div>

## Типы ошибок в словах
- Опечатки (случайный набор неверного символа):
  - *Матрос -> Мартос; Матрос -> Матрас; userinfo -> user info*
- Орфографические ошибки:
  - Ошибки при записи речи на слух
  - Транслитерационные ошибки (в иноязычных словах/именах собственных)
  - Когнитивные ошибки (смешение понятий)
  - *матрос -> мотрос; бодаться -> бадаться; предать -> придать*
- Ошибки распознавания:
  - *layer -> 1ayer; Щупальца -> Шупальца; Макушка -> Манушка*

<div style="page-break-after: always;"></div>

## Способы решения
Сперва рассмотрим способы решения спеллинга. Самый простой способ - поиск в словаре: для каждого слова в тексте проверяем его наличие в словаре. 
Однако, довольно сложно создать словарь, в котором будут все возможные слова и их формы (Имена собственные, заимствоания). Поэтому есть чуть более универсальные способы.

### N-граммы
По сути это кортежи размерности N чего-угодно: символов, слов, предложений, частей речи и пр. Примеры:
- Символьные триграммы: *матрос -> ##a, #ма, мат, атр, тро, рос, ос#, c##*
- Словные биграммы: *Мама мыла раму -> [BOS] Мама, мама мыла, мыла раму, раму [EOS]* 
- Биграмы признаков, например, POS-тэги: *Мама мыла -> OUT NOUN, NOUN VERB, VERB OUT*

Как их можно использовать для спеллинга:
- Построить словарь частотности символьных триграмм
- Для каждого слова взять его триграммы и проверить, что они входят в этот словарь и у них "достаточные" частоты. 
Предполагается, что, например, у триграммы "шкя" будет низкая частота либо её вообще не будет в слов
аре, что будет индикатором ошибки.

<div style="page-break-after: always;"></div>

## Устройство словаря
Далее рассмотрим как строить словарь и как его использовать для решения поставленной задачи.
- Самый простой способ - линейный список, но его минусы очевидны: линейный поиск, ну или логарифм, если список отсортирован. 
Слов в языке: 100-200к, у каждого с десяток форм. Уже получается довольно долго.
- Бинарное дерево поиска - не сильно лучше отсортированного массива + бин поиск
- Хеш-таблица - уже лучше. Поиск - амортизированная константа. Но остаётся проблема с размером:
```
200 000 слов * 15 форм на слово * 9 символов в слове * 2 байта на символ = 50 Мб
```
- Бор или префиксное дерево: вершины - символы слов, терминальные вершины -  слова. Плюс такой структуры в быстром поиске( О(длина слова) ) и очень маленьком размере( О(максимальная длина слова * мощность алфавита) ). Также эта структура удобна для поиска ближайших слов, но об этом позже. Далее пример.

<div style="page-break-after: always;"></div>

Входной словарь: *кот, кошка, кит, пёс*. Представление бором:

```mermaid
graph LR
    A1((#))-->B1((К))
    A1-->B2((П))
    B1-->C1((О))
    B1-->C2((И))
    B2-->C3((Ё))
    C1-->D1((T)):::red
    C1-->D2((Ш))
    C2-->D3((Т)):::red
    C3-->D4((С)):::red
    D2-->E1((К))
    E1-->F1((А)):::red
    classDef red fill:#f00;

```

В GitHub mermaid не работает, поэтому смотрим локально.
<div style="page-break-after: always;"></div>

## Особенности задачи
- Чаще всего ошибки локальны (затрагивают один-два символа)
- Может влиять и более широкий контекст: *ться -> цца, ant -> ent*
- Исправление опечаток требует поиска близких слов в словаре
  
Одной из оценок близости между двумя строками является расстояние Левенштейна (или же edit distance) - минимальное число замен, вставок и удалений, необходимых, чтобы из одной строки получить другую

Рассояние Левенштейна симметрично, неотрицательно и удовлетворяет неравенству: 

<img src="https://render.githubusercontent.com/render/math?math=\large \delta(u, v) \le \delta(u,w) %2B \delta(w,v)">
  

  Итак, как с помощью расстояния Левенштейна найти ближайшее слово к данному:
  - Перебрать весь словарь, сравнивания слова с данныи - **очень долго!** 
  - Генерируем строки, которые можно получить из данной за одну или две ошибки, ищем в словаре - уже чуть лучше, не порождаем заведомо далекие гипотезы.
  - Делаем это с помощью бора: спускаемся по нему и считаем кол-во замен, когда идем по вершине, не соответсвующую данному слову. Если на какой-то ветви ошибком много - отсекаем. 

При этом как выбрать кандидатов для исправления с одинаковым количеством исправлений? Для разных задач различные исправления имеют разный вес:
-  Опечатки: близкие символы на клавиатуре и обмен символов внутри слова должны иметь больший вес
-  Орфографические ошибки: гласные не меняются на согласные, мягкий знак в конце слова, типичные случаи *"f" -> "ph", "цца" -> "тся"* 
-  Ошибки распознавания: замены похожих символов графически
Ну и, конечно, веса замен можно обучать, например, на истории изменении Википедии.

<div style="page-break-after: always;"></div>

## Проблемы
Даже имея большой словарь, мы не застрахованы от ложных срабатываний, например, на редких терминах и именах собственных: *Эллочка -> Ёлочка*

Также ошибки возможны на словарных словах, когда, например, слово не подходит грамматически к данному предложению: *Отличный день, чтобы **расслабится** и ничего не делать. (расслабиться)* 

Композиты в некоторых языках, например в немецком, могут также усложнить задачу при сильном расчете на словарь: 
```
Donaudampfschiffahrtselektrizit?tenhauptbetriebswerkbauunterbeamtengesellschaft
```
Перевод: *Общество служащих младшего звена органа по надзору за строительством при главном управлении электрического обслуживания дунайского пароходства*

Ну а теперь предлагается на пройденном материале решить задачу исправлени опечаток в [spelling.ipynb](spelling.ipynb)

<div style="page-break-after: always;"></div>

## Grammatical Error Correction
Теперь рассмотрим более сложную задачу - исправление грамматических ошибок. Здесь уже внутри одного слова задачу не решить, т.к. слова с грамматическими ошибками могут быть в словаре, но при этом они не соответсвуют предложению, в котором находятся. 

Задача GEC обычно формулируется как задача исправления предложения. Т.е. система GEC на вход принимает предложение с потенциальным ошибками и должа вернуть исправленное предложение. Пример:

**Input:** *She see Tom is catched by policeman in park at last night.*

**Output:** *She saw Tom caught by a policeman in the park last night.*

Датасеты обычно включают в себя предложения или параграфы, а также указания ошибок со спанами в тексте, типом ошибки и как нужно исправить:

*...I have just <span style="color:red">recieved</span> the letter, which lets me know that I have won the first prize. I am proud of winning it and would like to say how <span style="color:red">thankful</span> I am... \
"edits": [71, 79, "received", "S"], [195, 203, "grateful", "RJ"]\
S – spelling error\
R - word or phrase needs Replacing\
J - AdJective*


<div style="page-break-after: always;"></div>

## Способы решения
Эту задачу обычно решают как задачу машинного перевода (SMT - statistical machine translation). А одним из  основных способов решения задачи SMT является seq2seq with attention. Т.е. входная последовательность токенов кодируется в энкодере, а затем декодер декодирует её в новую последовательность, обращая внимание только на определенные входные токены при генерации очередного выходного токена.

Этот подход для решения задачи GEC описан в статье ["A Multilayer Convolutional Encoder-Decoder
Neural Network for Grammatical Error Correction"](https://www.aaai.org/ocs/index.php/AAAI/AAAI18/paper/viewFile/17308/16137). Концептуально, здесь обычный seq2seq с механизмом внимания, только архитектуры энкодера и декодера весьма специфичны. Они осонованы на нескольких слоях сверткок и GLU (Gated Linear Unit), в то время как обычно в seq2seq используются рекуррентые сети.

Другой подход, который сейчас является SOTA на английском, основан не на seq2seq, а на sequence labelling, как мы смотрети в POS-tagging. Только тэги здесь - это одно из 5000 специфических правил замены, удаления, добавления или сохранения. Для этого был построен огромный словарь форм глаголов, а одни из тэгов означают "Заменить форму глагола на такую". Сама архитектура - просто энкодер берта с парой линейныйх слоёв и softmax. Более подробно подход описан в статье ["GECToR – Grammatical Error Correction: Tag, Not Rewrite"](https://arxiv.org/pdf/2005.12592.pdf)